%------------------------------------------------------------------------------
\chapter{Research Paper: Measuring the quality of relational to RDF mappings}
\label{sec:app_rdb2rdf}
%------------------------------------------------------------------------------

\section{Abstract}
Mapping between relational data and linked data (RDB2RDF) is an essential prerequisite for evolving the World Wide Web into the Web of Data.
We propose a methodology to evaluate the quality of such mappings against a set of objective metrics.
Our methodology, whose key principles have been approved by a survey among RDB2RDF experts, can be applied to evaluate both automatically and manually performed mappings regardless of their representation.
The main contributions of the paper are: (1) assessing the quality requirements for mappings between relational databases and RDF, and (2) proposing methods for measuring how well a given mapping meets the requirements.
We showcase the usage of the individual metrics with illustrative examples from a real-life application.
Additionally, we provide guidelines to improve a given mapping with regard to the specific metrics. 

\section{Introduction}

Translating the data stored in relational databases (RDB) to the linked data format is an essential prerequisite for evolving the current Web of documents into a Web of Data.
In order to be effectively and efficiently reusable, linked data should meet certain data quality requirements~\cite{Zaveri}.
Assessing the quality of a linked dataset created from RDB may be a laborious, repetitive task if the dataset is frequently recreated from its RDB source, e.g.\ after any update to the RDB.
We therefore claim that it is possible to positively influence the quality of a linked dataset created from RDB by improving the quality of the RDB2RDF \emph{mapping} that produces the linked data\footnote{Here, we use the term ``mapping'' to refer to the function that maps relational database columns to RDF properties, but not to materialisations of such mappings in concrete languages or representations, nor to tools that would execute such a function.}.
To the best of our knowledge there has not been prior research towards collecting and describing quality requirements for RDB2RDF mappings.
Thus, this paper aims to fill the gap by providing a system of metrics to measure the quality of RDB2RDF mappings.

A large number of tools for mapping relational data to RDF exist already\footnote{One of the listings is available at \url{https://github.com/timrdf/csv2rdf4lod-automation/wiki/Alternative-Tabular-to-RDF-converters}.}; they implement different mapping approaches, often allowing to customize the mapping.
To standardize the description of mappings, the W3C RDB2RDF Working Group has released two recommendations in 2012 (more details in~ Section\ref{sec:mapping-quality}): Direct Mapping~\cite{direct_mapping}, which produces RDF graph representations directly from a relational database (data and schema) and the Relational Database to RDF Mapping Language (R2RML)~\cite{r2rml} for expressing customized mappings.
 
In this paper, we aim at developing a system of quality metrics that can be applied both to direct and customized mappings that works with different representations of mappings (R2RML or proprietary formats, visual diagrams, tables, etc.).
Any of these representations are suitable for being evaluated with the proposed system as long as one can derive a list of database columns (including unmapped ones) and their corresponding RDF properties (for mapped ones).

To determine the scope of requirements that can be posed to RDB2RDF mappings, we studied the state of the art in related research fields such as ontology matching, linked data quality assessment and RDB2XML mappings. 
However, as Section~\ref{related_work} shows, these metrics collected from the scientific community do not solve the given problem entirely.
In order to fulfil the list, we followed the Direct and R2RML Mapping recommendations.
Some of the RDB2RDF features standardized in the documents are connected with the output data quality dimension, while others influence the quality of the mapping itself. 
For example, the ability to define R2RML views allows to incorporate domain semantics into the mapping – an opportunity that we consider to increase the quality of mapping.
We propose not only a system of metrics but also define means of measuring them, along with guidelines to improve the rating of a dataset w.r.t.\ each metric.

The paper is structured as follows: in Section~\ref{related_work} we discuss the existing approaches, in Section~\ref{requirements} we summarize the quality requirements for mappings and the metrics for measuring the quality.
\hyperref[evaluation]{Section~\ref*{evaluation}} presents the results of the survey conducted in order to collect the community feedback.
In Section~\ref{conclusions}, we conclude and propose future research directions.

\section{Related work}
\label{related_work}

\paragraph{RDB2RDF mapping approaches.}
Although previous research has not explicitly focused on collecting requirements for RDB2RDF mappings, many such requirements can be found in best practice and mapping approach descriptions.
For instance, in \cite{hillairet2008mde}, the importance of reusing existing vocabularies to enhance the interoperability of the output dataset is explained.
We extend this statement to a set of requirements combined in the \emph{interoperability} dimension, taking into account not only quantitative, but also qualitative metrics.

We notice that existing literature on the topic uses incoherent terminology.
Often, the term \emph{mapping} is used instead of mapping \emph{language}, e.g.\ in~\cite{Auer2010,Erling2008}. 
Discussing the requirements for mapping \emph{language}, both articles mention the following requirements for RDB2RDF \emph{mapping} as well:

\begin{itemize}
\item presence of both ETL and on-demand implementation
\item incorporating domain semantics that is often implicit or not captured at all in the RDB schema
\item indication of time metadata about the dataset during the mapping creation process to control the dataset currency (we subsume this metric under “data quality”)
\item intensive reuse of the existing ontologies
\end{itemize}    

\paragraph{Requirements for mappings between relational data and XML.}
The topic of mapping relational data to \emph{XML} is related to RDB2RDF mapping due to the similarity between the XML and RDF data models.
The earliest articles on the quality of RDB2XML mappings (e.g.~\cite{Schmidt, Shanmugasundaram}) only take into account the performance of the mapping algorithms.
This is because RDB2XML mappings are mostly produced automatically and do not allow customization.
Only few authors (e.g.~\cite{sahuguet2001everything}) propose metrics for evaluating the quality of the output data.
However, the proposed metrics evaluate only the syntactic correctness of the output XML and therefore cannot be applied to documents in RDF (except for \emph{presence of syntax errors}).

Other approaches (e.g.\ \cite{he1999relational}) prove the need for customizable techniques of mapping due to the wide gap between these two formats.
As their major evaluation criterion, they use the \textit{efficiency of query processing}.
We adopt this approach and base our \textit{simplicity} metric on it.
Liu et al.~\cite{liu2006designing} describe an approach to design a high-quality XML Schema from ER diagrams.
The authors define a list of requirements for the design; the most relevant one for our purposes is \emph{information preservation}.
However, the metrics for \emph{measuring} such requirements are not discussed.
We study the information preservation requirement in detail and provide objective metrics in the \emph{Faithfulness of Output} dimension.

\paragraph{Requirements for ontology matching.}
Mapping between a database schema and an RDF vocabulary can also be viewed as a special case of ontology matching.
Several studies propose requirements for measuring the quality of an ontology matching.
According to~\cite{euzenat2007ontology}, all proposed measures can be distinguished between compliance measures, measures concerned with system usability and performance measures focusing on runtime or memory usage.
As our study focuses on evaluating the quality of mapping itself, we do not take the system usability or performance dimensions into account.

In the evaluation of ontology matching, compliance measures are based on the two classical information retrieval measures of \emph{precision} and \emph{recall}~\cite{vanRijsbergen}.
We adapt these metrics to the evaluation of RDB2RDF mappings and include them in the \emph{faithfulness of output} dimension as \emph{coverage} and \emph{accuracy of data presentation} metrics. 

\paragraph{Measuring linked data quality.}
The quality of mapping correlates with the quality of output linked data produced by the mapping.
Poor design decisions made at the stage of defining a mapping, such as using deprecated classes, redundant attributes or badly formed URIs, decreases the quality of the output data.
Therefore, metrics for the quality of the output data can be viewed as metrics for quality of the mapping.

The field of assessing the quality of linked data is still in its infancy, but several articles have addressed it already. 
Our recent survey~\cite{Zaveri} collects and compares the existing papers, 
the most complete and detailed ones being Bizer's PhD thesis~\cite{bizer2007quality}, Flemming's master's thesis~\cite{flemming} and empirical evaluations by Hogan et al.\ \cite{hogan2010weaving, hogan2012empirical}. 
The survey provides formal definitions of data quality dimensions and metrics as well as evaluates existing tools for (semi-)automatic measurement of linked data quality.

The current paper assumes that the quality of a linked dataset is influenced by the mapping that produces it and thus categorizes the metrics from the survey from the perspective of the RDB2RDF mapping.
We select those metrics that are related to the mapping process and adapt them to the RDB2RDF domain.


\section{Quality Requirements}
\label{requirements}

This section gives a detailed overview of the proposed quality requirements for RDB2RDF mappings, ways of measuring them (metrics) and guidelines for improving mappings w.r.t.\ these metrics.
Our proposed system incorporates four quality dimensions with 14 objective metrics overall.
For assessing the overall quality of a mapping, one would, in practice, assign \emph{weights} to the metrics.
Their choice depends on the goal of the mapping process.
For example, when the goal of the mapping is to accurately represent the relational data,
the metrics in the “faithfulness of the output” dimension should be assigned the highest weight.

\begin{table*}[!ht]

	\centering		
	\begin{tabulary}{\textwidth}{LLL}
	\toprule
		\textbf{{Requirement}} & \textbf{{Description}} & \textbf{{Measure}} \\
		\midrule
		
		\multicolumn{3}{c}{\textbf{Quality of the mapping implementation and representation}} \\
\hline
		\textbf{Data accessibility} &
Describes how the mapping result can be accessed. 
		 & ETL/on-demand/both\\
\hline
		\textbf{Standard compliance} &
Characterizes if the mapping representation is standard compliant.
		 & boolean \\
\hline
		 
		\multicolumn{3}{c}{\textbf{Faithfulness of the output}} \\		
\hline
		\textbf{Coverage} & Characterizes the mapping completeness  & percentage of DB columns mapped  \\
\hline
		\textbf{Accuracy of data representation} & Characterizes the mapping correctness & percentage of correctly mapped DB columns \\
\hline
		\textbf{Incorporation of domain semantics} & Shows level of domain semantics incorporation & percentage of properties that link to the results of SQL queries \\
\hline
		
		\multicolumn{3}{c}{\textbf{Quality of the output}} \\		
\hline
		\textbf{Simplicity} & Shows the simplicity of SPARQL queries returning the frequently demanded values & percentage of complex SQL queries results integrated into the mapping \\
\hline
		\textbf{Data quality} & Characterizes the quality of output data & aggregation of linked data quality metrics ($\to$ \autoref{tab:data_quality_groups})\\
\hline
		\textbf{Data integration} & Characterizes the interlinking degree of the output data & percentage of external instances integrated into the resultant dataset\\	
\hline
		
		\multicolumn{3}{c}{\textbf{Interoperability}} \\		
\hline
		\textbf{Reuse of existing ontologies} & Shows the amount of reused vocabulary elements & percentage of reused properties and classes\\
\hline
		\textbf{Quality of reused vocabulary elements} & Characterizes the quality of chosen for reuse properties and classes & accumulated quality and popularity of reused vocabulary elements \\
\hline
		\textbf{Accuracy of reused properties} & Characterizes the accuracy of properties reuse & percentage of accurately reused properties \\
\hline
		\textbf{Accuracy of reused classes} & Characterizes the accuracy of classes reuse & percentage of accurately reused classes  \\
\hline
		\textbf{Quality of declared classes/properties} & Shows the quality of ontology documentation & accumulated quality of declared classes/properties\\
\hline
	
	\end{tabulary}
		
	\caption{Summary table of proposed metrics system}
	\label{tab:summarytable}
\end{table*}

\subsection{Quality of the mapping implementation and representation}
\label{sec:mapping-quality}
The requirement of mapping quality implementation and representation combines the requirements for resultant data accessibility and standard compliance of the mapping representation.

\paragraph{Data accessibility.}
Data accessibility describes how the result of the mapping is accessed. 
This metric is also known in the literature as “access paradigm”, “mapping implementation” or “data exposition”~\cite{spanos2012bringing}.
There are two possibilities: (i) Extract Transform Load (ETL) and (ii) on-demand mapping.
According to~\cite{Erling2008}, ETL means physically storing triples produced from relational data in an RDF store.
The disadvantage of ETL is that that, whenever the RDB is updated, you have to re-run the entire ETL process, even if just one RDB record has changed, carrying out an often redundant synchronization process.
However, in the ETL case nothing more than the RDF store is needed to answer a query.

On-demand mapping is realized by translating a SPARQL query into one or more SQL queries at query-time, evaluating these against (a set of) unmodified relational database(s) and constructing a SPARQL result set from the result sets of such SQL queries.
In contrast to the ETL implementation, on-demand mapping requires more resources for processing each query.
However, the on-demand implementation does not face the synchronization issue and does not replicate the data.  
In the light of these advantages and disadvantages, we claim that the best solution is to implement a mapping with both data access approaches.
Thus, the index of implementation takes a value equal to 1, if both implementations are present and 0.5 otherwise.


\paragraph{Standard compliance.}

An RDB2RDF mapping can either be represented as a set of customized rules or through a generic set of rules (as defined by the W3C Direct Mapping standard~\cite{direct_mapping, Bertails_2011}).
Often, the output of a Direct Mapping may not be useful, that is, it may not adequately take the structural/semantic complexity of the database schema into account.
Thus, while the applicability of a Direct Mapping satisfies the requirements for the “standard representation” metric, not using the customized rules will lead to a loss of points w.r.t.\ other metrics.

Languages for representing customized mappings have been surveyed in~\cite{spanos2012bringing}.
Until recently, no standard representation language for RDB2RDF mappings existed, however, as of 2012, R2RML~\cite{r2rml} has been released as a W3C recommendation.
As it is the only mapping language that has been standardized to date, we do not consider other (proprietary) formats reasonable.

Thus, we propose that if there is no material representation of a mapping available, it should be assumed that a Direct Mapping is carried out.
We define the “standard representation” metric to be one if the mapping is represented in \emph{R2RML} or if there is no representation (and therefore a Direct Mapping is applicable), and zero otherwise.
 
\subsection{Faithfulness of the output}
In terms of RDB2RDF mapping we define the faithfulness of output as an abstract measure of similarity between the source data and resultant dataset.

\paragraph{Coverage.}
This metric indicates the ratio of the database columns mapped to the RDF. 
In general, a high coverage increases the faithfulness of the output data.
However, reaching the highest level of coverage may conflict with privacy restrictions.
Therefore, great care should be taken when mapping any kind of personal (sensitive) data that might be linked with other data sources or ontologies.
It may even be illegal to create or to process such linked information in countries with data protection laws without having a valid legal reason for such processing~\cite{bechhofer2004owl}.
 
Moreover, application-specific data such as statistics of usage or service tables often are not included into the mapping due to its application-related nature.
Thus, in real-world applications, the coverage metric never reaches one.
%scale

\paragraph{Accuracy of data representation.}
When mapping relational data to RDF, the accuracy of data representation is tightly connected to data formatting issues.
For example, differences in the representation of numeric values between the database and RDF can cause inaccuracies in the output data.
Inaccurate dealing with not-Latin characters leads to loss of data meaning. 
We propose the following algorithm to evaluate the accuracy index of the mapping:

\begin{enumerate}
\item\label{it:avg-database} for each mapped database column containing numeric data, compute the average value of the numbers in this column;
\item find an average value for the corresponding property in the RDF output, and compare it to the average computed in step~\ref{it:avg-database};
\item\label{it:num_points} add a point for each coinciding average (considering the statistical error)
\item\label{it:lit_points} for each mapped database column containing literal data, analyze the readability of the corresponding properties; add a point for each completely readable property;
\item calculate a sum of the points obtained in steps~\ref{it:num_points} and~\ref{it:lit_points} and divide the sum on the total number of columns mapped.
\end{enumerate}

\paragraph{Incorporating domain semantics.}
\label{par:dom_semantic}

The incorporation of domain semantics is one of the crucial aspects that indicate the quality difference between direct and customized mappings. 
Measuring this requirement helps to estimate the direct benefit of mapping customization.
Generally, a direct mapping does not incorporate domain semantics; in this case the metric takes a value of zero. 
The extent of domain semantics incorporation can be measured by counting number of \emph{class properties} which take values from any database table \emph{other than} the table that corresponds to a class.
The metric is then calculated as the ratio between this number and the  count of properties summed up over all classes in the mapping.

To improve the mapping w.r.t.\ this metric, implicit relations between the data in the database should be explicitly modelled in the mapping.
This process also increases the simplicity of the mapping, as it requires integration of the SQL query results into the mapping, thereby simplifying (future) SPARQL queries for obtaining these values. 

\subsection{Quality of the output}

Quality of the output combines requirements of the output data quality in aspects of simplicity of usage, level of interlinking and objective data quality metrics.

\paragraph{Simplicity.}
One often has the task of mapping highly complex data models to RDF.
In that case, Direct Mapping produces an RDF output that may be difficult to use, especially when considering the limitations of the SPARQL query language.
Thus, useful information from the source database may lose its value in the RDF dataset that results from mapping.
Metrics for the quality of the output should therefore take the simplicity of the RDF dataset into account.
By simplicity of RDF data, we mean the simplicity of \emph{operating} on it.
In other words, the simplicity of data determines the simplicity (or length in terms of abstract syntax) of the SPARQL queries returning frequently demanded values.  
To increase the simplicity, the mapping should aim to produce a dataset, that can be queried for frequently demanded values by relatively simple SPARQL constructions. 
To do this, the \emph{frequently demanded values returning by the complex SQL queries} should be integrated into the mapping.
Such preparations not only increase the simplicity of the output, but also \emph{incorporate domain semantics} not presented in the relational database explicitly (cf. Section~\ref{par:dom_semantic}).

To calculate the index of simplicity, first a list of complex SQL queries should be assembled.
We propose to consider a query “complex” in the following cases:
\begin{itemize}
\item a query joins at least two tables or views,
\item a query supposes recursive computations over one table or view.
\end{itemize}
After the list is assembled, frequently demanded values, returned by the queries should be selected.
This selection is subjective and should be carried out by a group of developers and administrators of the project.
The simplicity metric is then calculated as the percentage of \emph{frequently demanded values returned by complicated SQL queries} that have been integrated into the mapping. 

\paragraph{Data quality.}
\label{data_quality_par}
The quality of a mapping can partly be assessed by evaluating the quality of the output RDF data.
Section~\ref{related_work} points to approaches for assessing data quality.
However, when evaluating the quality of a mapping, not all aspects of output data quality need to be taken into account.
This is due to the fact that not all aspects of linked data quality are affected by the mapping stage.

\begin{table}[!tb]
\scriptsize
\centering
\begin{tabular}{p{0.52\linewidth}|p{0.46\linewidth}}
\toprule
\multicolumn{2}{c}{\textbf{Requirements for linked data quality and their metrics}} \\

\midrule
\myparagraph{1. Influenced by mapping}
\begin{itemize}
\item \textbf{Completeness:} influenced by coverage
\item \textbf{Validity-of-documents:} influenced by quality and accuracy of reused and newly determined classes
\item \textbf{Interlinking:} influenced by data integration 
\item \textbf{Availability:} influenced by dereferenceability of reused and newly defined classes
\item \textbf{Consistency:} influenced by reuse
\end{itemize}



\myparagraph{2. Depend on mapping} 
\begin{itemize}
\item \textbf{Provenance:} indication of metadata about a dataset
\item \textbf{Verifiability:} verifying publisher information, authenticity of the dataset, usage of digital signatures
\item \textbf{Licensing:} machine-readable/human-readable license, permissions to use the dataset, attribution, Copyleft or ShareAlike
\item \textbf{Validity-of-documents:} no syntax errors
\item \textbf{Consistency:} entities as members of disjoint classes, usage of homogeneous datatypes, misplaced classes or properties, misuse of \texttt{owl:datatypeProperty} or  \texttt{owl:objectProperty}, bogus \texttt{owl:Inverse-FunctionalProperty} values, ontology hijacking
\item \textbf{Conciseness:} redundant properties/instances, not-unique values for functional properties, not-unique annotations
\item \textbf{Performance:} no usage of slash-URIs, no use of prolix RDF features
\item \textbf{Understandability:} human-readable labelling of classes, properties and entities by providing \texttt{rdfs:label}, human readable metadata
\item \textbf{Interpretability:} misinterpretation of missing values, atypical use of collections, containers and reification
\textbf{Currency:} currency of data source
\item \textbf{Volatility:} no timestamp associated with the source
\end{itemize}


&

\myparagraph{3. Depend on relational data}
\begin{itemize}
\item \textbf{Completeness:} values for a property are not missing
\item \textbf{Amount-of-data:} see \cite{Zaveri}
\item \textbf{Provenance:} trustworthiness of RDF statements, trust of an entity, trust between two entities, trust from users, assigning trust values to data/-sources/rules, trust value for data
\item \textbf{Believability:} see \cite{Zaveri}
\item \textbf{Accuracy:} see \cite{Zaveri}
\item \textbf{Consistency:} no stating of inconsistent property values for entities, literals incompatible with datatype range
\item \textbf{Interpretability:} interpretability of data
\item \textbf{Versatility:} provision of the data in various languages
\end{itemize}

\myparagraph{4. Depend on publishing} 
\begin{itemize}
\item \textbf{Availability:} accessibility of the server, accessibility of the SPARQL end-point, accessibility of the RDF dumps, no structured data available, no dereferenced back-links
\item \textbf{Performance:} see \cite{Zaveri}
\item \textbf{Security:} see \cite{Zaveri}
\item \textbf{Response-time:} see \cite{Zaveri}
\item \textbf{Conciseness:} keeping URIs short
\item \textbf{Understandibility:} indication of one or more exemplary URIs, indication of a regular expression that matches the URIs of a dataset, indication of an exemplary SPARQL query, indication of the vocabularies used in the dataset, provision of message boards and mailing lists
\item \textbf{Versatility:} provision of the data in different serialization formats, application of content negotiation
\item \textbf{Currency:} see \cite{Zaveri}
\item \textbf{Timeliness:} see \cite{Zaveri}
\end{itemize}


\\
\bottomrule
\end{tabular}
\caption{Data quality metrics divided into four groups according to the stage of mapping creation, on which they can be influenced.}
\label{tab:data_quality_groups}
\end{table}

We confine ourselves to the objective requirements discussed in~\cite{Zaveri} and divide them into four groups (cf.~\autoref{tab:data_quality_groups}):

\begin{itemize}
	\item Requirements that are influenced by mapping quality implicitly.
		Increasing the quality of the mapping in turn increases the quality of data.
	\item Requirements that are influenced by mapping explicitly.
		The proposed data quality index discussed below aggregates these metrics.
	\item Requirements that depend on the quality of data stored in the database. 
		We consider these metrics to be out of the scope of this paper.
	\item Requirements that depend on the quality of linked data publishing.
		We do consider these metrics to be out of the scope of this paper.
\end{itemize}

The metrics and methods of measuring them are discussed in detail in~\cite{Zaveri}.
To evaluate the quality of mapping with regard to output data quality we propose to aggregate only the metrics explicitly depending on the mapping.
There are two possible strategies for calculating our overall data quality metric based on the individual metrics: assigning the same weight to all individual metrics, or assigning different weights to different dimensions or even to individual metrics.

\paragraph{Data integration.}
The key advantage of Linked Data is its ability to effectively integrate data from disparate, heterogeneous data sources.
In most cases, the output dataset resulting from a mapping can be linked to existing datasets via explicitly modelled relationships between entities.
As mentioned in~\cite{sahoo2009survey}, the use of domain ontologies along with user defined inference rules for reconciling heterogeneity between multiple RDB sources, is an effective integration approach for creating an RDF. 
A simple metric for data integration can be defined as the number of external datasets linked to the one evaluated.
However, this metric cannot be mapped to a value in the interval $[0,1]$ in an obvious way, as needed for computing the overall mapping quality from the values of the individual metrics.
Thus, we propose an alternative measure to evaluate the data integration level: index of data integration.
It can be calculated as the ratio between the number of external instances integrated in the resulting RDF dataset and the total number of instances.

\subsection{Interoperability}
\label{inter}

The interoperability requirement tackles the aspect of a mapping to produce interoperable data, which can be easily linked to and integrated with other ontologies and datasets.

\paragraph{Reuse of existing ontologies.} 
The reuse of existing ontology elements (i.e.\ classes and properties), increases the interoperability of the output dataset.
Additionally, this reuse prevents one from having to introduce new elements. 
This requirement can be measured using two metrics: the ratio of reused properties and the ratio of reused classes.
However, not only the quantity, but also the \emph{quality} of reused properties and classes is important.
This aspect is covered by the next metric.

\paragraph{Quality of reused vocabulary elements.}
\label{par:quality_reused}
Measuring the quality of reused properties or classes is not trivial.
As a large number of vocabularies has been published on the Web, many vocabulary elements are repeatedly declared instead of being reused.
We claim that the best choice of a class or property to reuse depends on two parameters:
(i) the quality of the class or property itself and 
(ii) the frequency of its usage in LOD in comparison with semantically similar alternatives.
Thus, to measure the quality of each individual property or class we propose the following workflow: 
\begin{enumerate}
\item Measure the quality of the chosen vocabulary element (class/property)
\item Find alternatives to the chosen element and measure their quality
\item Compare the quality metrics of the chosen element and the best alternatives
\end{enumerate}

We propose to compute the quality index ($i_\mathit{qual}$) of the chosen vocabulary element as the product of the following two indexes:
the index of documentation quality ($i_\mathit{doc}$) and the index of popularity ($i_\mathit{pop}$).
The proposed index of documentation quality is computed differently for classes and properties.
Both for classes and properties, the index of documentation quality takes into account dereferenceability as well as documentation by \texttt{rdfs:label} and \texttt{rdfs:comment} (preferably in multiple languages).
If the class or property is explicitly deprecated, i.e. its \texttt{owl:deprecated} annotation property equals \texttt{true}, we define its quality index as zero.
Additionally, for classes, their relation to other classes should be indicated (rdfs:subClassOf or owl:equivalentClass).

For properties, \texttt{rdfs:range} and \texttt{rdfs:domain} should be defined.
Based on the presence of the properties definitions mentioned above, a decision about quality of the documentation for a vocabulary element (possibly automated) should be made.
The index of popularity aims to measure the frequency of usage of the class or property on the Web in comparison with semantically similar alternatives.
This index can be taken from services such as Linked Open Vocabularies\footnote{http://lov.okfn.org}.

As an example, we evaluate the property \texttt{agrelon:hasChief}\footnote{full URI: \texttt{http://d-nb.info/standards/elementset/agrelon.owl\#hasChief}} that links an \emph{organization} to its \emph{chief}.
The property is dereferenceable, it has labels available in four languages; however, no \texttt{rdfs:range}, \texttt{rdfs:domain} or \texttt{rdfs:comment} is specified.
Based on this, we define its $i_\mathit{doc}$ as 0.5.
For measuring the frequency of usage we type the term `chief' into the search field of the Linked Open Vocabularies service. 
Looking through the results, we can conclude that on the Web of Data the term `chief' is most commonly used to model a military commander and the chosen property \texttt{agrelon:hasChief} has a low popularity metric value of 0.271.
Thus, the quality index for this evaluated property is $i_\mathit{pop} * i_\mathit{doc} = 0.271 \cdot 0.5 = 0.1355$.

The next task is to find the most commonly used term for the evaluated property.
The list of possible substitutes will include properties matching `chief', `leader', `head', or `boss'.
For each item on the list the property with the highest $i_\mathit{pop}$ and satisfying the following requirements, should be found:
\begin{itemize}
\item \texttt{rdfs:comment}, \texttt{rdfs:domain} and \texttt{rdfs:range} statements should correspond to the domain model
\item depending on the application settings, special requirements such as presence of an inverse property in the same vocabulary may need to be satisfied. 
\end{itemize}

In our case, the metrics for the best candidates are represented in Section~\ref{sem_identical}.
Due to space limitations, we only take into account the first two synonyms from that list, as the two other ones do not have matches that satisfy the requirements given above.
According to the investigation, the \texttt{agrelon:hasChief} property is not the best possible alternative for linking an organization to its chief.
The \texttt{swpo:hasLeader} property should be used instead, as its quality index is $i_\mathit{pop} \cdot i_\mathit{doc} = 0.682 \cdot 0.8 = 0.5456$.

\begin{table}[!htb]
\centering
\begin{tabulary}{\columnwidth}{L|L|L}

\toprule
& \textbf{`chief'} & \textbf{`leader'}  \\
\midrule
best candidate & agrelon:hasChief & swpo:hasLeader  \\
\textbf{$i_\mathit{pop}$} & 0.271 & 0.682  \\
\textbf{$i_\mathit{doc}$} & 0.5 & 0.8 \\
\textbf{$i_\mathit{qual}$} & \textbf{0.1355} & \textbf{0.5456} \\
\end{tabulary}
\caption{Metrics for semantically identical properties to link an organization and its chief}
\label{sem_identical}
\end{table}

The last step is to compare the quality of the chosen property and the best possible alternatives.
The resultant score for the chosen property is calculated as the ratio between its quality index and the quality index of the best possible alternative: $i_{\mathit{qual}_\mathit{chosen}} / i_{\mathit{qual}_\mathit{best}}$ where $i_{\mathit{qual}_\mathit{best}}$ is the quality index of the best alternative and $i_{\mathit{qual}_\mathit{chosen}}$ is the quality index of the evaluated property.
In our case, choosing the \texttt{agrelon:hasChief} property adds $0.1355/0.5456 = 0.248$ points to the interoperability metric.
%Similarly, the quality of all reused properties and classes should be calculated.
Thus, the overall value of the \emph{quality of reused vocabulary elements} metric is defined as the ratio between the sum of the quality indexes of all reused vocabulary elements and the number of reused vocabulary elements.
 
\paragraph{Accuracy of reused properties.}
We determine two requirements for accurate reuse of existing properties: 
(i) respecting the property definition and 
(ii) unambiguous meaning of the property.
Respecting the property definition means that the domain model must be consistent with the definition of the property, namely its \texttt{rdfs:range}, \texttt{rdfs:domain} and \texttt{rdfs:comment} relations.
Unambiguous meaning is important when similar relations are presented in the domain model.
For example, consider the domain model of our SlideWiki OpenCourseWare authoring platform\footnote{\url{http://www.slidewiki.org}} and the following relations between decks (collections of slides): 
\begin{itemize}
\item \verb|Deck1  sw:isTranslationOf  Deck2| 
\item \verb|Deck2  sw:isRevisionOf     Deck3|
\item \verb|Deck3  sw:isVersionOf      Deck4|
\end{itemize} 

Here, \texttt{Deck2} results from \emph{translating} \texttt{Deck1} into a different language, \texttt{Deck3} is a result of \emph{editing} \texttt{Deck2}, and \texttt{Deck4} results from \emph{moving} \texttt{Deck3} into another e-learning platform (for example, Moodle\footnote{\url{http://moodle.org}}).  
If we decide to reuse the property \texttt{dcterms:isVersionOf} instead of one of these three properties from SlideWiki's domain-specific vocabulary, the meaning of the property will be ambiguous. 
A better solution in this case is to declare all three new properties to be sub-properties of \texttt{dcterms:isVersionOf}.

The index of accuracy of reused properties can be calculated as the number of accurately reused properties (i.e. the properties, that satisfy both requirements) in relation to the total number of reused properties in the dataset.

\paragraph{Accuracy of reused classes.} 
An important aspect of reusing classes is the semantic \emph{compatibility}.
By this we mean the compatibility of the meaning of a reused class and a domain object declared to be an instance of the class.
The index of accuracy of reused classes is defined as the ratio between semantically compatible reused classes and the total number of reused classes.
We illustrate the issue with an example below.

Let us assume that we need to link a deck of slides to its CSS style.
The property \texttt{oa:styledBy} from the \emph{Open Annotation Data Model ontology}\footnote{\url{http://www.w3.org/ns/oa}} could be chosen to model the link.
However, its domain is \texttt{oa:Annotation}, which is not compatible with the \texttt{sw:Deck} class in terms of its intended semantic.
If such a statement occurs within the mapping, the reused class \texttt{oa:Annotation} should be considered as incompatible for reuse.

%There are two possible semantically correct solutions:
%\todo{fix as in the questionnaire}
%\begin{itemize}
%\item \texttt{sw:styledBy rdfs:domain sw:Deck\\
%sw:styledBy rdfs:range sw:Style \\
%sw:styledBy rdf:relatedTo oa:styedBy \\
%sw:Style rdfs:subClassOf oa:CssStyle }
%
%\item \texttt{sw:styledBy rdfs:domain sw:DigitalObject\\
%sw:Deck rdfs:subClassOf sw:DigitalObject \\
%oa:Annotation rdfs:subClassOf sw:DigitalObject \\
%oa:styedBy rdfs:subClassOf sw:styledBy\\
%sw:styledBy rdfs:range sw:Style \\
%sw:Style rdfs:subClassOf oa:CssStyle \\
%}
%\end{itemize} 
%The first mapping does not reuse the \texttt{oa:styledBy} directly, however, the reasoner will find the connection.
%We propose to see the case still as a reuse of an existing property.
%Although the second statement is doubtful, we consider this way to be possible and semantically correct.
To measure the requirement we propose to calculate the index of the accuracy of reused classes.
It can be calculated as the number of reused classes semantically compatible with domain objects in relation to the total number of reused classes.

\paragraph{Quality of declared classes/properties.}
In order to obtain high quality, the classes/properties whose \emph{definition} is \emph{introduced} by the mapping should meet the requirements of documentation quality analogically to reused vocabulary elements (see Section~\ref{par:quality_reused}).
Thus, the proposed index of quality of declared vocabulary elements is computed separately for class and properties and accumulates the documentation quality score in relation to the total number of properties/classes declared.

\section{Evaluation}
\label{evaluation}
In order to evaluate how well the proposed methodology agrees with real-life common practices, we  conducted a survey using a questionnaire\footnote{\url{http://slidewiki.org/application/questionnaire.php}}.
The survey was formed of statements, one per requirement from Section~\ref{requirements}.
Each requirement was phrased in the style of a practical guideline, in that complying with this guideline when designing a mapping would help the mapping to obtain the highest possible score for the corresponding metric.
For example, the requirement ``accuracy of reused properties'' (Section~\ref{inter}), evaluated by combination of two metrics, was divided into two statements: ``The properties chosen for reuse should be available, dereferenceable and have unambiguous meaning in the application domain.'' and ``When choosing the properties for reuse, \textit{rdfs:domain}, \textit{rdfs:range} and \textit{rdfs:comment} must be taken into account.''
For each statement the experts had to express to what extent they agreed with it.
Thus, the agreement of a majority of experts with a statement would prove the relevancy of both the requirement and the proposed way of its measuring.
We did not include requirements for \emph{output data quality}, \emph{coverage} and \emph{accuracy of data} in the survey, as their metrics had been proposed by prior research. 
In addition to the opinions about the statements we collected open feedback.

\begin{table*}[!ht]
\scriptsize
\centering
\begin{tabulary}{\columnwidth}{p{0.2\linewidth}|
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.6}}C
|>{\columncolor[gray]{0.8}}C
|>{\columncolor[gray]{0.8}}C
|>{\columncolor[gray]{0.8}}C
|>{\columncolor[gray]{0.8}}C
|>{\columncolor[gray]{0.8}}C
|C}

\toprule
\textbf{Requirement} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{P4} & \textbf{P5} & \textbf{P6} & \textbf{P7} & \textbf{P8} & \textbf{P9} & \textbf{P10} & \textbf{P11} & \textbf{P12} & \textbf{P13} &\textbf{Mode}\\
\midrule
\textbf{Data accessibility} &4 &2 &5 &5 &4 &5 &5 &4 & 4 &2 &4 &2 &5 & 5, 4\\
\textbf{Representation} &2 &2 &4 &5 &1 &5 &4 &4 &3 &4 &3 &2 &5 &4 \\
\textbf{Incorporation of domain semantics} &4 &3 &3 &4 &3 &3 &3 &4 &2 &5 &5 &3 &5 &3 \\
\textbf{Simplicity} &3 &3 &3 &4 &3 &3 &4 &4 &5 &2 &1 &4 &4& 4, 3\\
\textbf{Data integration} &4 &3 &4 &3 &5 &2 &5 &4 &3 &4 &2 &4 &3 & 4 \\
\textbf{Reuse of existing ontologies} &5 &3 &4 &3 &5 &4 &5 &4 &5 &2 &2 &4 &2 & 5\\
\textbf{Quality of reused properties (frequency)} &4 &4 &4 &3 &4 &3 &4 &4 &3 &2 &2 &4 &2& 4 \\
\textbf{Quality of reused properties (dereferenceability)} &5 &1 &3 &4 & 5&3 &5 &4 &5 &4 &4 &4 &4 & 4\\
\textbf{Accuracy of reused properties} &4 &4 &3 &4 &5 &4 &5 & 4&4 &4 &3 &4 &5 & 4\\
\textbf{Accuracy of reused classes} &5 &4 &4 &3 &4 &4 &4 &3 &4 &4 &3 &4 &5 & 4\\
\textbf{Quality of declared classes/properties} &4 &3 &3 &4 &2 &4 &5 &4 &5 &2 &2 &4 &4 & 4\\
\bottomrule

\end{tabulary}
\caption{Results of survey on degree of agreement with the key concepts of the proposed methodology. Numbers represent the degree of agreement from \emph{absolutely agree} (5) to \emph{absolutely disagree} (1). The colors indicate the self-estimated level of expertise in the RDB2RDF domain from \emph{expert} (darkest) to \emph{experienced} (lightest). }
\label{survey}
\end{table*}

The survey was announced to the RDB2RDF community via mailing lists and personal e-mails.
We received 13 individual responses.
All participants assessed their level of RDB2RDF expertise as either \textit{experienced} or \textit{expert} (the two highest out of four possible levels).
They could decide either to stay incognito or to fill in their name, affiliation and institution.
Section~\ref{survey} shows the results.

Due to the high level of participants' experience in the domain, we chose not to leverage the individual opinions by calculating their means or medians. 
Instead, we base our analysis on the \emph{mode value}.
We did not calculate separate mode values for responses from participants with different experience levels (``experienced'' vs.\ ``expert''), as we do not consider the difference between these two groups sufficiently significant to influence the responses (however, we indicate the level of experience in the result table).  

As Section~\ref{survey} shows, the experts approved most of the requirements. 
The ones that the experts were doubtful about were the \emph{incorporation of domain semantics} and \emph{simplicity} requirements.
This outcome can, however, be explained by the circumstance that these two requirements are difficult to explain in a short statement. 
On the other hand, there were experts who accepted both these requirements, even with a value of \emph{absolutely agree}.
Thus, we consider the low overall agreement with these two requirements to result from incomplete understanding of the statements, as the requirements and their metrics are not trivial.
Finally, within the open feedback no participant suggested further objective requirements (besides ones aggregated in the data quality requirement), which provides evidence of the completeness of our system, at least in view of today's state-of-the-art.

\section{Conclusion}
\label{conclusions}
In this paper, we proposed a methodology to evaluate the quality of mappings between relational and linked data. 
In particular, we proposed a set of 14 requirements for mapping quality and described ways of measuring them.
We believe that the most of the measures can be done (semi-)automatically and will attempt to prove that in our future work.
As we show in the paper, the proposed system can not only be used to evaluate the mappings, but also as a guideline to increase the quality of the mapping.
Additionally, we evaluated the relevance of our proposed set of requirements by conducting a survey. 
A total of 13 experienced individuals, mainly from the RDB2RDF community, participated in our survey.  The analysis of their responses allows us to claim that the community accepts our requirements and their metrics.

\section*{Acknowledgements.}
We would like to thank the anonymous experts as well as Richard Cyganiak (INSIGHT@NUI Galway), Ravi Kumar (TCS), Mr.\ Spenser Kao (Bureau of Meteorology, Australia), Dr.\ Nikolaos Konstantinou (National Technical University of Athens), Freddy Priyatna (Ontology Engineering Group), Juan Sequeda (University of Texas at Austin), Tim Lebo (Tetherless World/RPI), Oscar Corcho (UPM), Dr.\ Pedro Szekely and Jose Luis Ambite (University of Southern California).
We are grateful to Mr.\ Nicolas Van Peteghem for providing us with one of the test databases.


%------------------------------------------------------------------------------
\chapter{Research Paper: Balanced scoring method for multiple-mark questions}
\label{sec:app_balanced}
%------------------------------------------------------------------------------

\section{Abstract}
Advantages and disadvantages of a learning assessment based on multiple-choice questions (MCQs) are a long and widely discussed issue in the scientific community.
However, in practice this type of questions is very popular due to the possibility of automatic evaluation and scoring.
Consequently, an important research question is to exploiting the strengths and mitigate the weaknesses of MCQs.
In this work we discuss one particularly important issue of MCQs, namely methods for scoring results in the case, when the MCQ has several correct alternatives (multiple-mark questions, MMQs).
We propose a general approach and mathematical model to score MMQs, that aims at recognizing guessing while at the same time resulting in a balanced score.
In our approach conventional MCQs are viewed as a particular case of multiple-mark questions, thus, the formulas can be applied to tests mixing MCQs and MMQs.
The rational of our approach is that scoring should be based on the guessing level of the question.
Our approach can be added as an option, or even as a replacement for manual penalization.
We show that our scoring method outperforms existing methods and demonstrate that with synthetic and real experiments.

\section{Introduction} 
\label{sec:intro}
Advantages and disadvantages of a learning assessment based on multiple-choice questions (MCQs) are a long and widely discussed issue in the scientific community.
However, in practice this type of questions is very popular due to the possibility of automatic evaluation and scoring~\cite{Farthing1998}.
Consequently, an important research question is to exploit the strengths and mitigate the weaknesses of MCQs.
Some systems (e.g. Moodle~\footnote{\url{https://moodle.org/}}) allow teachers to create MCQs with multiple correct options.
This type of questions we will call multiple-mark questions (MMQs), to distinguish them from the conventional MCQs, where there is always only one correct option.
Multiple-mark questions were already recommended by Cronbach~\cite{Cronbach}.
Other research~\cite{Ripkey1996,Pomplun1997,Hohensinn2011} considers MMQs to be more reliable, when compare them with conventional MCQs.
However, even though the advantages of MMQs are meanwhile widely accepted, up to our knowledge there are no balanced methods for scoring multiple-mark questions available to date.

One possible approach to score the MMQs is to use dichotomous scoring system. 
The dichotomous scoring awards the constant amount of points, when the question is answered correctly and zero points in a case of \textit{any} mistake.
However, the partial scoring is preferable to the dichotomous, especially in case of MMQs.~\cite{Ripkey1996,Jiao2012,Bauer2011,Ben-Simon1997} 

The second possible approach is to use the methods, developed for scoring the multiple true-false questions (MTFs).
However, despite the possibility to convert the MMQs into MTFs, the studies \cite{Cronbach,Dressel1953} show the differences between two formats.
Moreover, the researches mentioned above named the following disadvantages of MTF questions compared to MMQs:

\begin{itemize}
	\item The multiple true-false format "clouds" from the learners the possibility of marking several options as true.
	\item The level of reliability in multiple true-false questions is not equal for true and false answers.
	\item The multiple true-false format requires more resources to store the answers.
\end{itemize}

In the paper we show that the differences prevent the applying methods developed for the MTFs to the MMQs scoring.

Another possible approach is to use the penalties, similarly to the paper-based assessment where the teacher can analyze the student answers and decide how much points she deserves.
The method was proposed by Serlin~\cite{Serlin1978}.
For example, in Moodle a teacher has to determine what penalty applies for choosing each distractor.
However, this work is an additional, unpopular burden for teachers, since not required in paper-based tests.
Instead of asking the teacher, some systems calculate the penalties automatically. 
However, computer-based assessment opens additional possibilities to guess, for example choosing all options.
Often the scoring algorithms do not take into account such ways of guessing.

Consequently, we are facing the challenge to find a scoring method, that is able to recognize and properly penalize guessing.
Previously proposed algorithms suffer from imbalance and skewness as we show in \autoref{sec:related}.

The task to find the scoring method can be divided into two steps:

\begin{enumerate}
	\item Find a method to determine points for the correctly marked options.
	\item Find a method to determine the penalty for the incorrectly marked options.
\end{enumerate}

For the first part a reasonable approach was proposed by Ripkey~\cite{Ripkey1996}.
Thus our research aims to provide a method for the second part (determining penalties).
We propose a general approach and a mathematical model, that takes into account the most common ways of guessing and behaves balanced at the same time.

Our concept is based on the assumption, that scoring can be based on the guessing level of the question.
Each question is associated with a difficulty to guess a (partially) correct answer.
To accommodate the difficulty level of guessing in the scoring method, we propose to determine the penalty only when a student marks more options, than the actual number of correct ones.
We argue that our approach can be added as an option, or even as a replacement of manual designation of penalties.
We claim that our algorithm behaves better, than existing ones and prove that with both synthetic and real experiments.
In our approach conventional MCQs are viewed as a particular case of multiple-mark questions, thus, the formulas can be applied to the tests mixed of MCQs and MMQs.
As the scoring of conventional MCQs is a trivial task, we do not consider such type of questions in our experiments.

The paper is structured as follows:
First, we present the terminology we use.
Then we discuss existing algorithms for scoring MMQs, which we have found in the research literature and real applications.
After that we describe our approach on conceptual and mathematical levels.
Finally we show and discuss the results of synthetic and real-life experiments.

\section{Terminology} %0.5
\label{sec:termin}

In the following we define the key concepts building the basis for our MMQ scoring method:

\textit{Dichotomous scoring} -- the concept of scoring the results, that allows users to get either the full amount of points or zero in a case of any mistake;

\textit{Partial scoring} -- the concept of scoring the results in a way that allows users to obtain some points for a question, which they answered only partially correct;

\textit{Difficulty} -- a difficulty weight of the question in the questionnaire in the interval $(0,1]$.
The difficulty can be determined automatically and dynamically based on prior scoring.
In our implementation, for example, difficulty is dynamically updated after one student provided an answer, according with the formula: 
	\[d' = \frac{incorr}{all}\]
In a case of dichotomous scoring, the values of $incorr$ and $all$ mean, respectively, the accumulated number of \textit{incorrect} and \textit{all} responses on the question by any user.
In a case of partial scoring, the definition of $incorr$ changes as follows:
	\[incorr = \sum_{i}1-d_{i}\]
where $i$ is a counter from 1 to the number of attempts for the question and
$D_{i}$ is the difficulty, that the question had at the moment, when the $i^{th}$-attempt was made.
After the difficulty is determined, it is scaled to the interval $(1,d_{max}]$, where $d_{max}$ is the maximal difficulty, that a question can have.
	\[d = f(d') = (d' *(d_{max}-1)+1 \]
The scaling is performed for better usability.
For example, $d_{max}$ can be set to 10 to obtain a difficulty level between 1 and 10.

\textit{Guessing level} -- the theoretical probability to guess the correct answer from the list of options.
In partial scoring, we determine the guessing level as the probability to obtain more than zero points.

\textit{Basic question points} -- an absolute value of points for the correctly checked options or the percentage of correctly checked options within all correct options.
Basic points = $f(d)$.

\textit{Penalty} -- the value, that should be deducted from the basic points due to the logic of the applied algorithm.
		In our approach we propose, that penalty should be only deducted, when user checks more options, than the number of correct ones.

\textit{Total question points} -- the amount of points for the question, gained by the user after the deduction of penalty.
		Total question score = $f(p,s)$.

\section{Related work}
\label{sec:related}

There are several existing platforms, that use multiple-mark type of questions as well as several approaches to score them. 
We collected such approaches to describe, discuss and compare them.
Existing approaches for scoring the multiple-mark questions implement four base concepts.
In the section we describe the basic ideas, advantages and disadvantages of these concepts.

\subsection{Dichotomous scoring}

This method is often used in paper-based questionnaires, where the good quality of questionnaires allows teacher to be more strict when score the results.
In the case choosing a wrong option indicates, that a student hopes to guess the correct response as she does not know the material behind the question well.
In e-based learning the quality of questionnaires is not perfect, especially in the systems with collaborative authoring.
That is why the dichotomous scoring can punish the learners for the teachers mistakes too much. 
As the aim of questionnaires is not only to score the results, but to catch the gaps of knowledge, the scoring of partially correct responses shows the actual knowledge of the student better.
Also, dichotomous scoring does not show the accurate progress of the student.
However, when dealing with multiple-mark questions dichotomous scoring almost excludes the possibility of guessing, that is why we use it as a standard of reference when evaluating our approach with real users.

\subsection{Morgan algorithm}
One of the historically first methods for scoring the MMQs was described in the 1979 by Morgan \cite{Morgan1979}.
In the accordance to the method, the scores are determined by the following algorithm:

\begin{enumerate}
    \item for each option chosen which the setter also considers correct, the student scores +1.
    \item for each option chosen which the setter considers to be incorrect, the student scores -1.
    \item for each option not chosen no score, positive or negative, is recorded regardless of whether the setter considers the response to be correct or incorrect.
\end{enumerate}

The algorithm can be improved by changing the constant 1 to dynamically determined amount of points:

\begin{enumerate}
    \item for each option chosen which the setter also considers correct, the student scores $+(p_{max}/n)$, where $n$ is a number of correct options
    \item for each option chosen which the setter considers to be incorrect, the student scores $-(p_{max}/k)$, where $k$ is a number of distractors.
\end{enumerate}

We use this improved algorithm for our experiments.
However, the experiments show a large dependence between number of options (correct and incorrect) and amount of penalty, that indicates the skewness of the method (see \autoref{subsec:experiment}).

\subsection{MTF scoring}

Multiple-mark questions can be scored with the approaches developed for multiple true-false items.
The base approach to score the MTF items is to determine, how close is the student response to the correct one.
Tsai~\cite{Tsai1993} evaluated six different implementations of the approach.
Later his findings were confirmed by Itten~\cite{Itten1997}.
Although both researches found partial crediting to be superior to dichotomous scoring in a case of MTFs, they do not consider any of the algorithms to be preferable. 
This fact allows us to use the most base of them for our experiments.

All the MTF scoring algorithms imply that any item has $n$ options and a fully correct response is awarded with full amount of points $p_{max}$.
If the user did not mark a correct option or marked a distractor, she is deducted with the penalty $s = p_{max}/n$ points.
Thus a student receives points for not-choosing a distractor as well as for choosing a correct option.
This point does not fit perfect to multiple-mark questions because of the differences between two types~\cite{Pomplun1997,Cronbach,Frisbie1992}.
Our experiments (see \autoref{subsec:experiment}) confirm the studies and show the skewness of the concept when deal with MMQs.
The main problem of the MTF scoring method, when applied to MMQs, is that a user obtains points, even if she did not chose any options. 
Although the problem can be solved by creating an additional rule,
the experiments show the further problems of the algorithm, when used for MMQ items. 


\subsection{Ripkey algorithm}

Ripkey~\cite{Ripkey1996} suggested a simple partial crediting algorithm, that we named by the author.
In the approach a fraction of one point depending on the total number of correct options is awarded for each correct option identified.
The approach assumes no point deduction for wrong choices, but items with more options chosen than allowed are awarded zero points.

The Ripkey's research showed promising results in a real-life evaluation.
However, later researches (e.g. Bauer~\cite{Bauer2011}) notice the limitations of the Ripkey's study.
The main issue in the Ripkey algorithm is the not well-balanced penalty.
Our experiments show that in many cases the algorithm penalizes so severely, that learners could consider it to be the dichotomous scoring. 
We aim to improve the Ripkey's algorithm by adding the mathematical approach for evaluating the size of penalty.

\section{Balanced scoring method for MMQs}
\label{sec:method}

\subsection{Concepts} %0.5
\label{subsec:concepts}

As shown above, existing approaches do not solve the problem of scoring MMQs perfectly.
Our concept is based on the assumption, that scoring can be based on the guessing level of the question.
Thus, when a student marks all possible options, she increases the guessing level up to 1. 
In this case the student should obtain either the full amount of points (if all the options are considered to be correct by the teacher), or zero, if the question has at least one distractor. 
However, if a student did not mark any option, the score should be always zero, as we assume that all the questions have at least one correct option.
Thus, the task is to find the correctness percentage of the response and decrease it with a penalty, if the guessing level was artificially increased by marking too many options.

Questions have the native level of guessing, and we propose to deduct the penalty only if after the student's response the guessing level increases.
In other words, we determine the penalty only when a student marks more options, than the number of correct ones.

\subsection{Mathematical model} %1.5
\label{subsec:math_model}

In this section we present the mathematical model as well as an algorithm, that can be used for its implementation. 

\subsubsection{Assumptions and restrictions}
\label{subsec:restrictions}

We propose to use our approach only in systems, that comply with the following requirements for assessment items:

\begin{itemize}
  \item all the item's options have the same weight;
  \item there is at least one correct option;
  \item there are no options excluding all other (e.g. "all above are correct")
\end{itemize}
\begin{figure}[h!]
	\centering
		\includegraphics[width=.5\columnwidth]{images/algorithm.jpg}
	\caption{Flow chart of the Balanced scoring algorithm.}
	\label{fig:algorithm}
\end{figure}

\subsubsection{Scoring the basic points}

To score the basic points we use the approach, described by Ripkey. 
Below we present it mathematically in accordance with the following designations:

\begin{itemize}
    \item $d \in \mathbb{R} , d \in (1..d_{max}]$ -- difficulty of the current question, for our experiments we set $d_{max} = 5$ 
    \item $C \subseteq A$ -- set of the \textit{correct} options $c_i$ for the current question, where $A$ -- set of the options $a_j$ for the current question,
    \item $c_{max} = |C| , c_{max} \in \mathbb{N}$ -- number of \textit{correct} options for the current question
    \item $C_{ch}$ -- set of the \textit{correctly checked} options
    \item $c_{ch} = |C_{ch}| , c_{ch} \in \mathbb{N} , c_{ch} \in [0,c_{max}]$ -- number of \textit{correctly checked} options for the current question
    \item $p_{max} = f(d) = d*K_{points}$ -- maximal possible points for the current question, in our system we set $K_{points} = 1$
    \item $p_c$ -- points for the correctly checked option $c$. 
    As we assume all the correct options have the equal weight, 
    \[\forall c \in C_{ch} \vert p_c = \frac{p_{max}}{c_{max}}\]
    \item $p \in \mathbb{R} \wedge p \in [0,p_{max}]$ -- the basic points for the current question,
    \[p = \sum_{c \in C_{ch}}{p_c} \Rightarrow \]

   \[p = \sum_{c \in C_{ch}}{\frac{p_{max}}{c_{max}}} = \frac{p_{max}}  {c_{max}}*c_{ch} = p_c*c_{ch}\]
\end{itemize}

\subsubsection{Scoring of the penalty}

Below we present our approach for scoring the penalty.
We use the following designations:

\begin{itemize}
    \item $a_{max} \in \mathbb{N} , a_{max} = |A|$ -- number of options $a \in A$
    \item $Ch \subseteq A$ -- set of \textit{checked} options
    \item $ch = |Ch| , ch \in \mathbb{N} , ch \in [0,a_{max}]$ -- number of checked options for the current question
    \item $b \in \mathbb{R} , b \in [0,1]$ -- basic level of guessing for the current question, 
    \[b = \frac{c_{max}}{a_{max}}\]
    \item $n \in \mathbb{R} , n \in [b,1]$ -- measure, that shows the possibility, that user tries to guess the correct response by choosing too much options; we do not evaluate it in the cases, when $n <= b$,
   \[n = \frac{ch}{a_{max}}\]
    \item $s$ -- penalty for the guessing,
    \[s = n-b \Rightarrow s \in [0,1-b]\]
    \item $s_k \in [0,p_{max}]$ -- the penalty, mapped to the maximal possible points.
\end{itemize}

A mapping function is calculated as follows:
\[f : s_k \rightarrow s\]
Given, $s_k \in [0,p_{max}]$ and $s \in [0,1-b]$, then 
\[f : s_k \rightarrow s = f : [0 , {1-b}] \rightarrow [0,p_{max}] \Rightarrow\]
\[s_k = f(s) = s*\frac{p_{max}}{1-b} = (n-b)*\frac{p_{max}}{1-b}\]

\subsubsection{Scoring the total question score}

The absolute score for the question is trivially determined as
\[T = f(p,s_k) = p-s_k \]
The percentage representation of the total score is determined as follows:
\[T_\% = \frac{p - s_k}{p_{max}}*100\%\]


\section{Evaluation}
\label{sec:experiments}

\subsection{Synthetic experiments}
\label{subsec:experiment}

In the subsection we describe our experiments with synthetic data and compare the behavior of different methods.
For shorter presentation, we use the following reductions:

\begin{itemize}
  \item{Dich.} -- dichotomous scoring;
  \item{Balanced} -- the proposed balanced scoring method
\end{itemize}

We consider all the questions to have the difficulty $d = 1$, then the maximal possible points $p_{max} = 1$ as well.

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c}  
	\toprule  
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case1.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.4&0&0&0\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 1}
\end{table}

\begin{example}[Case: 5 options, 2 correct, 5 marked]
In the case the student chose all the options and should obtain zero points. 
However, we see that MTF method does not recognize this type of guessing and considers the questions to be answered partially correct, awarding the points for two correct options, that were marked.
\end{example}

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c} 
	\toprule  
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case2.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.6&0&0&0\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 2}
\end{table}

\begin{example}[Case: 5 options, 2 correct, 0 marked]
The situation is opposite to the previous: in the case the student chose none of the options. 
As we assume that question must have at least one correct option, in  case of not choosing any options a student also should obtain zero points.
However, we see that MTF method awards the points for three distractors, that were not marked.
Although the situation is absurd, we faced it within real learning platforms, for example within several on-line courses of the Stanford University~\footnote{http://online.stanford.edu/courses}.
\end{example}

Two examples below are trivial and the problem could be solved by adding the rules. 
However, the MTF scoring also suffers from skewness, when applied to MMQs, as it is shown below.

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c} 
	\toprule  
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case3.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.83&0.5&0.5&0.5\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 3}
\end{table}

\begin{example}[Case: 6 options, 2 correct, 1 correct marked]
This case proves, that the MTF method has a dependency from a number of correct and incorrect options. 
Thus, in a case of 6 options two of which are correct, a student is awarded 0.833 points for choosing only one correct option.
In a case of 5 options two of which are correct, she would be awarded 0.80 points for the same.
Moreover, if she choose only one incorrect option in a case of 6 alternatives, she obtains 0.5 points; in a case of 5 options she will be awarded 0.4 for the same.
\end{example}

Thus, our experiments prove, that multiple-mark questions can not be scored properly with the algorithms, developed for multiple true-false items.
Moreover, a teacher should be careful when creating multiple true-false questions and create them in such a manner, that not-choosing a distractor deserves awarding.
However, the MTF scoring is the only existing approach of partial scoring that can be used in a case, when a question does not have any correct options.

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c} 
	\toprule 
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case4.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.5&0&0.5&0.5\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 4}
\end{table}

\begin{example}[Case: 4 options, 2 correct, 1 correct and 1 incorrect marked]
This case illustrates the issues of using the Morgan algorithm.
The Morgan algorithm deducts penalties for choosing the incorrect option, as well as the proposed approach.
There are two main issues:

\begin{itemize}
  \item Does the response deserve penalty?
  \item If deserves, how big the penalty should be?
\end{itemize}

In that case we are facing the situation, that penalty has the same size, as the basic points, and the student is awarded zero.
We consider the penalty to be needlessly high, especially because the penalty depends on the number of incorrect options.
Thus, if the question has 3 incorrect options, choosing one of them would be fined on 0.33, and in case of 2 incorrect options, the penalty is 0.5.
After recognizing behavior of the algorithm, students will mark only the options, they are sure in, because choosing an incorrect one may cost them a full amount of points, they collected with correct options.
\end{example}

The next two examples show mainly the differences between the proposed approach and Ripkey algorithm.
Namely, we show the situations, when Ripkey algorithm awards zero points, while we consider that it should award more. 

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c} 
	\toprule  
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case5.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.75&0.5&0&0.5\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 5}
\end{table}

\begin{example}[Case: 4 options, 2 correct, 2 correct and 1 incorrect marked]
In this case the student chose more options, than the number of correct ones, and according to the Ripkey, the answer should be awarded zero.
Our claim is, that until the student have not chosen all the options, she could have some points.
However, choosing three of four options could mean a try of guessing.
Although in this case the student gets the full amount of \textit{basic} points, she is fined on a half of them.
\end{example}

\begin{table}[h!]
	\centering
%	\renewcommand{\tabcolsep}{0.15cm}
%	\renewcommand{\arraystretch}{1.3}
	\begin{tabularx}{0.5\columnwidth}{c c c c c} 
	\toprule  
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case6.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.8&0.67&0&0.67\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 6}
\end{table}

\begin{example}[Case: 5 options, 2 correct, 2 correct and 1 incorrect marked]
The example shows the disadvantage of the Ripkey algorithm more clear.
It is not clear for the student, why she was awarded zero points, as she did not try to guess and answered partially correct.
\end{example}

\begin{table}[h!]
	\centering
	\begin{tabularx}{0.5\columnwidth}{c c c c c}    
    \toprule
    \multicolumn{5}{c}{\includegraphics[width=0.5\columnwidth]{images/case7.jpg}}\\
    \midrule
    \textbf{Dich.}&\textbf{MTF}&\textbf{Morgan}&\textbf{Ripkey}&\textbf{Balanced}\\
	\midrule
    0&0.6&0.17&0.67&0.67\\
	\bottomrule
    \end{tabularx}
	\caption{Comparison of the proposed approach with other existing approaches}
	\label{tab:case 7}
\end{table}

\begin{example}[Case: 5 options, 3 correct, 2 correct and 1 incorrect marked]
In that case balanced scoring and Ripkey algorithms behave the same, as none of them deducts a penalty.
\end{example}

\section{Conclusions}
\label{sec:conclusions}

In the paper we evaluate the existing approaches for scoring the multiple-mark questions and propose a new one.
The proposed approach has a list of restrictions, however it has advantages when compare with the discussed approaches.
One of the main advantages is its clearness for the students, that was proven by the user evaluation.
Also, our approach is based on the mathematical model, it does not suffer from the skewness, as it has the same formula for all cases.
At the same time, the proposed approach recognizes the attempts to guess the correct answer, for example choosing all the possible options.
When compare with the existing approaches, the advantages of the proposed algorithm could be summarized as follows:

\begin{itemize}
  \item The approach allows to score both multiple-mark and conventional multiple-choice questions.
  \item The approach is based on the partial scoring concept.
  \item The algorithm can be easily implemented, it is pure mathematical.
  \item The score does not highly depend on the amount of correct and incorrect options.
  \item The value of the penalty is in balance with the possibility, that the student is trying to guess.
  \item Due to the balance, the results are clear for the students.  
\end{itemize}

However, we suppose our algorithm to be optional together with other discussed approaches.
This is due to the fact, that teachers create questions in their own manner and should be able to choose an appropriate method to score the results.
Also, the different situations require different levels of 	
severity, and the proposed approach might be too lenient.




%------------------------------------------------------------------------------
\chapter{RDB2RDF Mapping Documentation}
\label{sec:app1}
%------------------------------------------------------------------------------
\section{Extractions from R2RML mapping representation}
\label{app1:r2rml}

\begin{lstlisting}

prefix rr: <http://www.w3.org/ns/r2rml#>.
@prefix sw: <http://slidewiki.org/rdf/sw#>.
@prefix wa: <http://slidewiki.org/rdf/wa#>.
@prefix sa: <http://slidewiki.org/rdf/sa#>.
@prefix dcterms: <http://purl.org/dc/terms/>.
@prefix foaf: <http://xmlns.com/foaf/0.1/>.
@prefix cnt: <http://www.w3.org/2011/content#>.
@prefix oa: <http://www.w3.org/ns/oa#>.


<#Deck>
	rr:logicalTable [ rr:sqlQuery """
		SELECT deck.language, deck_revision.*
		FROM deck
		JOIN deck_revision ON deck_revision.deck_id = deck.id """
	];
	rr:subjectMap [
		rr:template "http://slidewiki.org/rdf/deck/{id}";
		rr:class sw:Deck;
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:created;
		rr:objectMap [ rr:column "timestamp" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:language;
		rr:objectMap [ rr:column "language" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:title;
		rr:objectMap [ rr:column "title" ];
	];    
	rr:predicateObjectMap [
		rr:predicate sw:popularity;
		rr:objectMap [ rr:column "popularity" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:description;
		rr:objectMap [ rr:column "abstract" ];
	];
	rr:predicateObjectMap [
		rr:predicate wa:instanceOf;
		rr:objectMap [ 
		rr:parentTriplesMap <#DeckContainer>;
			rr:joinCondition [
				rr:child "deck_id";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:creator;
		rr:objectMap [ 
		rr:parentTriplesMap <#User>;
			rr:joinCondition [
				rr:child "user_id";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:contributor;
		rr:objectMap [ 
		rr:parentTriplesMap <#User>;
			rr:joinCondition [
				rr:child "user_id";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate wa:translationOf;
		rr:objectMap [ 
			rr:template "http://slidewiki.org/rdf/deck/{tr}";
		];
	];
	rr:predicateObjectMap [
		rr:predicate wa:revisionOf;
		rr:objectMap [ 
			rr:template "http://slidewiki.org/rdf/deck/{based_on}";
		];
	];
	rr:predicateObjectMap [
		rr:predicate oa:styledBy;
		rr:objectMap [ 
			rr:template "http://slidewiki.org/rdf/style/{def}";
		];
	].
	
	
<#Slide>
	rr:logicalTable [ rr:sqlQuery """
		SELECT slide.language, slide_revision.*
		FROM slide
		JOIN slide_revision ON slide_revision.slide = slide.id """
	];
	rr:subjectMap [
		rr:template "http://slidewiki.org/rdf/slide/{id}";
		rr:class sw:Slide
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:created;
		rr:objectMap [ rr:column "timestamp" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:language;
		rr:objectMap [ rr:column "language" ];
	];
	rr:predicateObjectMap [
		rr:predicate cnt:chars;
		rr:objectMap [ rr:column "content" ];
	];    
	rr:predicateObjectMap [
		rr:predicate sw:popularity;
		rr:objectMap [ rr:column "popularity" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:description;
		rr:objectMap [ rr:column "comment" ];
	];
	rr:predicateObjectMap [
		rr:predicate sw:slideNote;
		rr:objectMap [ rr:column "note" ];
	];
	rr:predicateObjectMap [
		rr:predicate wa:instanceOf;
		rr:objectMap [ 
		rr:parentTriplesMap <#SlideContainer>;
			rr:joinCondition [
				rr:child "slide";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:creator;
		rr:objectMap [ 
		rr:parentTriplesMap <#User>;
			rr:joinCondition [
				rr:child "user_id";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate sw:translator;
		rr:objectMap [ 
		rr:parentTriplesMap <#User>;
			rr:joinCondition [
				rr:child "translator_id";
				rr:parent "id";
			];
		];
	];
	rr:predicateObjectMap [
		rr:predicate wa:translationOf;
		rr:objectMap [ 
			rr:template "http://slidewiki.org/rdf/slide/{tr}";
		];
	];
	rr:predicateObjectMap [
		rr:predicate wa:revisionOf;
		rr:objectMap [ 
		rr:template "http://slidewiki.org/rdf/slide/{based_on}";
		];
	].
	
		
<#Question>
	rr:logicalTable [ rr:tableName "questions" ];
	rr:subjectMap [
		rr:template "http://slidewiki.org/rdf/question/{id}";
		rr:class sw:Queston;
		rr:class sa:Queston;
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:created;
		rr:objectMap [ rr:column "timestamp" ];
	];
	rr:predicateObjectMap [
		rr:predicate sw:defaultDifficulty;
		rr:objectMap [ rr:column "difficulty" ];
	];
	rr:predicateObjectMap [
		rr:predicate cnt:chars;
		rr:objectMap [ rr:column "question" ];
	];
	rr:predicateObjectMap [
		rr:predicate sa:difficultyLevel;
		rr:objectMap [ rr:column "diff_count" ];
	];
	rr:predicateObjectMap [
		rr:predicate wa:revisionOf;
		rr:objectMap [ 
			rr:template "http://slidewiki.org/rdf/question/{based_on}";
		];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:creator;
		rr:objectMap [ 
			rr:parentTriplesMap <#User>;
			rr:joinCondition [
				rr:child "user_id";
				rr:parent "id";
			];
		];
	].
	
	
<#User>
	rr:logicalTable [ rr:tableName "users" ];
	rr:subjectMap [
		rr:template "http://slidewiki.org/rdf/user/{id}";
		rr:class sw:User;
	];
	rr:predicateObjectMap [
		rr:predicate foaf:mbox;
		rr:objectMap [ rr:column "email" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:nick;
		rr:objectMap [ rr:column "username" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:date;
		rr:objectMap [ rr:column "registered" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:firstName;
		rr:objectMap [ rr:column "first_name" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:familyName;
		rr:objectMap [ rr:column "last_name" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:language;
		rr:objectMap [ rr:column "locale" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:gender;
		rr:objectMap [ rr:column "gender" ];
	];
	rr:predicateObjectMap [
		rr:predicate sw:speaks;
		rr:objectMap [ rr:column "languages" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:based_near;
		rr:objectMap [ rr:column "location" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:topic_interest;
		rr:objectMap [ rr:column "interests" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:depiction;
		rr:objectMap [ rr:column "picture" ];
	];
	rr:predicateObjectMap [
		rr:predicate sw:homeLocation;
		rr:objectMap [ rr:column "hometown" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:birthday;
		rr:objectMap [ rr:column "birthday" ];
	];
	rr:predicateObjectMap [
		rr:predicate dcterms:description;
		rr:objectMap [ rr:column "description" ];
	];
	rr:predicateObjectMap [
		rr:predicate foaf:page;
		rr:objectMap [ rr:column "infodeck" ];
	].
    
\end{lstlisting}

%\section{Tabular mapping representation}
%
%\begin{figure}[!b]
%	\centering
%		\includegraphics[width=\columnwidth]{images/mapping_pages1.pdf}
%		\caption{Matching the DB tables and columns to WikiApp ontologie classes and properties}
%	\label{fig:wa_matching_1}
%\end{figure}
%
%\begin{figure}[!b]
%	\centering
%		\includegraphics[width=\columnwidth]{images/mapping_pages2.pdf}
%		\caption{Matching the DB tables and columns to WikiApp ontologie classes and properties (cont.)}
%	\label{fig:wa_matching_2}
%\end{figure}
%
%\begin{figure}[!b]
%	\centering
%		\includegraphics[width=\columnwidth]{images/mapping_pages3.pdf}
%		\caption{Matching the DB tables and columns to WikiApp ontologie classes and properties (cont.)}
%	\label{fig:wa_matching_3}
%\end{figure}


\section{SlideWiki knowledge graph}


\begin{sidewaysfigure}[b]
    \centering
    \includegraphics[width=\columnwidth]{images/knowledge_graph.png}
    \caption{SlideWiki Knowledge graph}
	\label{fig:knowledge_graph}
\end{sidewaysfigure}




